icon:material/pipe
# Pipeline Workflow: Modifying a preset repo to handle an unexpected developments

By the time I trained out several versions of UDW, I got familiar enough with the process where I was able to make changes as needed whenever something would arise in the process. Whether changing up the dataset creation process, or how I was automating the process as I got experienced enough to figure out where I can fit in pieces to reduce my workload. I learned a lot of python along the way and more about the inner workings of model training. As stated before, I got a jump start using [Anime Screenshot Pipeline](https://github.com/cyber-meow/anime_screenshot_pipeline) but ultimately my end result in changing the pipeline workflow looks unrecognizable to even the developer's current version of the process.

??? note
    
    -A standard anime episode of 24 minutes is around 34k frames
    -A standard film of 2 hours is around 170k frames
    -Each frame when exported through ffmpeg is around 1.5MB-3MBs in size
    -This makes the size of frame extractions 300GBs for a single episode and 1.5TB for a movie. 
    -Cutting the opening and ending animation off each episode as well as the next episode preview knocks off 195 seconds or 4680 frames, which would be between 7-14GB. Cutting the movie credits removes around 6 minutes or 8,640 frames, or 14-28GB.
    -A manually cleaned up episode will come down to about 50GBs in size and a movie will drop to around 600GB 
    -Lots of Hard Drive space required

## Frame Extraction 
The first change I made to the pipeline was the ffmpeg script provided. The Pipeline used a script that would run the mpdecimate command to remove duplicate or “frozen frames”, moments in the animation where there is no movement, as ffmpeg is “extracting” the frames to condense down the frames to a final output around a 10th of the original size. 
However, I came to find out much later there was a large spread of false positive frame removals for unknown reasons and false negatives of frames that were kept from a combination of mpdecimate not considering jump cuts that repeat scenes after several cuts, and due to the Blu-ray encodings creating enough pixelation changes between frames that it would defeat mpdecimate’s deletion threshold. 
Because the loss of data was too significant to ignore, I stopped using mpdecimate and let ffmpeg extract every single frame and began using a deduping script that would sort duplicates based on the image’s file hash value and would only move all the flagged frames to a different folder instead of deleting them, allowing for manual review in the future. The downside to this method is that I am now using significant amount of hard drive space which would eventually force me to start buying hard drives and to looking into NAS solutions to hold all this data. It did also present a way to schedule when to start manual reviewing the datasets, as this would free almost enough space to frame extract another episode.  

## Tagging
Stable Diffusion base models use CLIP captioning to tag their datasets with natural language for prompt keywords. NovelAI’s anime finetune of SD1.4 instead used the English based Japanese imageboard site Danbooru’s tagging format as parent company Anlatan scrapped Danbooru for all the images used in training their model and kept the metadata. Thus, all future anime models created by enthusiasts either finetuning that model or attempting to “finetune” SDXL base models to do better anime generations would follow this format when classifying their info. Thus, most Stable Diffusion dataset classifiers created follow the Danbooru tag format, even for realistic models.

I initially used the built in Automatic1111 tagger when starting out making embeds and LoRAs and worked fine for the standard images you would find on the internet. The problem comes when working on 16:9 images, all the classifiers out in the wild were trained mostly on 1:1 aspect images due to SD1.x being trained on 512x512, and whatever sizable collection of images trainer could collect in other resolution sizes for regularization. This particular resolution would confuse the classifiers and generate an absurd amount of false positive tagging of character subjects onto all sorts of images that were not character focused; a scenery shot, item focus shots, transitioning scenes, logos and texts, panning of the environment before a subject walks into the shot, a magical explosion on screen. This incorrect info would result in generations where the subject would not appear in the image, or would appear fused into parts of a scenery, special effects shots, or just shots of empty hallways and unprompted backdrop focused images. 
The solution I ended up coming up with was repurposing a face detection tool used in the original github to generate 1:1 cropped images for a dataset. After tagging my dataset, I would run a modified version of the script that will duplicate all images that pass the face detection threshold into a different folder. and when I would run the dataset into my image organizer Hydrus, I would delete all the subject tags from those images and then import the copied images with the accurate subject tags. 
My first training after using this method instantly fixed the prompting of subjects. A quick skim of the images that passed the face detection threshold show no signs of false positives, and there is no issue if there are cases of false negatives, as manual review will take care of anything that were missed. Other miscellaneous improves I used for this phase include a classifier aggregate that will tag based on the average of 3 or more classifiers running concurrently and has the tagging of copyright names removed just so that I reduce false positive tags of characters that don’t belong in the content from being erroneously introduced. 

## Dataset Organization and Preparation
While dedicated image organizers for stable diffusion datasets are available, I elected with using an obscure desktop application called Hydrus Network. Created in the early 2010s for the purpose of organizing large media collections (of internet memes and other s#@tposts) under a single location with various customizable categories modeled after the format of imageboards such as danbooru. The media is tabulated based on file hash rather than whatever the file is named as. This aspect synergizes not only with how the datasets need to be tagged if following the Booru/NovelAI format, but with how my deduping script operates on file hash values when sorting unique frames out, as well as how I incorporate face detection copies of images with correct subject tagging.

Once my sorted and tagged images are completed, I will import the dataset batch into Hydrus and it will associate the tag sidecar txt files generated by the taggers to the images and will automatically populate the datapoints hits. I also include Hydrus specific metadata of the series, episode, and scene for later manual review once the import is finished.
Once the first import is complete, I will select the entire batch and edit their tag information to remove all the subject tags (1boy/1girl and other variations of multiple of each for example) in a single click, and then import the copied images from the face detection step. Since the copies have the same hash value, the only thing that will change is that the sidecar txt values will update to include any new values and will not replace any changes I may have already made. Once the subject values have been reintroduced, I will then delete those copies, but will keep the originals of the first import as they will still be required in the future.
From here I can proactively check any tags I may have previously had issues with and search for all images by that tag, remove tags if incorrect, maybe even delete images that could be seen as bad data, and overall just skim that the frames I did get were satisfactory.
I will repeat this process with all new dataset batches I make until the model is ready to train. From here I can use the Hydrus metadata I included to only select a specific range of data that is ready to go and it will export a new set of copies with sidecar txt files that the trainer will need to associate the tags and images.

## Kohya Trainer
Things to note before continuing:
-Card used for training is a water cooled RTX4090 MSI Suprim Liquid X on my personal machine.
-Training 200k Images at 512x512 resolution at batch size 16 takes 48 hours. 
-Projected Training of 400k images at 768x768 resolution at batch size 1 is 120 hours based on a fellow trainer who trained a similar configuration with a 16GB Google TensorGPU on the cloud, with approximately 168 hours if trained at 1024x1024 over SD1/NAI. 
--From a test I already tried a 768 resolution, batch size at 16 caused a CUDA Out of memory error within minutes of training start and then requires a PC restart. Have not tried again with the current pending training data size. 
--SDXL training requires even more memory to accurately finetune the model, outside the scope of my hardware.
-While training the model, I cannot perform any other GPU based tasks on my machine or risk causing an Out of Memory error. It also gets very hot.
-Due to the above, I focused more on gathering the dataset and do as much quality work as I could before committing to a very lengthy training session.

Japanese developer Kohya-SS’s SD Script package is an old but very consistent way training package. While it mostly supports LoRA and other network-based checkpoint trainings, it still supports full finetune support for SD1 models. It only needs to be pointed to a training directory, will check the main training folder and the regulation folder if enabled, and will then just follow the training parameters set in the powershell script and output the checkpoint when done. It borrows from the NAI training settings and incorporates the aspect ratio bucketing so the resolution sizes are not restricted to 1:1 aspect image

## Post Training Review
After the model is trained, I will run several “templates” to test changes between a previous version of the model compared to the current one. I will note if there are any visual improvements, changes in the look of backgrounds, character feature consistencies, detail in non-upscaled generations, and if any errors I caught in the previous cycle were fixed. I will also take prompts of other stable diffusion images I see in the wild to test how others would prompt on this model in hopes of finding tags prompts that don’t aren’t producing intended results so I can then note down to check on Hydrus. 

