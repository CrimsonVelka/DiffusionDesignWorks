icon:fontawesome/solid/photo-film
# Motion Models â€“ AnimateDiff + UDW
As Text to Video has taken its first steps into mainstream with products such as Luma, interest in making this kind of AI content has grown, and there have been some groups that have made attempts in making video models for local applications in the open source space that integrate with Stable Diffusion.  

AnimateDiff is a plug-and-play module that will take T2I prompts and use the motion model to create short animation video clips with Stable Diffusion. This was created back in June 2023.

One of the issues as to why this never picked up much steam, aside from its incompatibility with SDXL at the time, is that there was no clear way or instruction to make the video generations do what you want. Had to bruteforce your way into a result and sometimes you just settled for what you got. The hardware requirements were also much higher, most people could only make 256x256 to about 384x384 clips and the generations would take 10 to 15 minutes to generate. They also could not be upscaled without running out of memory. While fortunate enough to have a 4090 and able to generate 768x512 clips, they still took a significant amount of time to train and were still prone to failing mid generation if there was some sort of hiccup for the bleeding edge tech at the time.

Sometime this year, the developers disclosed their training method and there has been at least one big finetune fork where the training resolution was doubled, but no new data was added due a lack of video tagging tools. With the visibility of the video dataset now clear, the path is open for those that have the hardware to finetune the motion model. The dataset, WebVid10M, is comprised of low-resolution watermarked previews of shutterstock footage. With the understanding of the model now clear, I can take my existing pipeline for SD and modify the roadmap to incorporate new video clips.

## Theoretical Pipeline

Since specific tuning is not required, we are open to using more types of movies, recordings, and other animations not necessarily related to official films and shows or even the ones we are using. Say we have access to a collection of videos. What we can do is use a script that will generate the timestamps like a chapter select on a BluRay disc, and mark every time there is a jumpcut to a different camera shot or scene and is output into a sidecar txt file. Then with ffmpeg, use the clip command in conjunction with the sidecar txt to generate clips based on the timestamps created. These clips will then be tagged and captioned and can be organized within Hydrus (Hydrus also can organize video format files) and use the same image organization tools to manually review tags and captions every time we need to train the model. 
An addition we could do that can enhance the way our finetune is trained is to introduce the Danbooru/Novel AI tag format as well as keeping the natural language captioning to better prompt what we want out of the motion model without clashing with the limited caption instructions trained to attempt to prompt actions. A method to do this would be to take a single frame from the peak of each clip, run the SD Tagger on it, and then apply those tags onto its corresponding video clip on Hydrus. This would be something that would need trial and error before creating an automated process.
From there we would do as we do with Stable Diffusion, but this time we will be using a video model trainer and can retrain the model from scratch if we so choose to do improvements on the tags, otherwise will finetune the model as we already do with Stable Diffusion.

In short:
Create scene jump cut timestamp sidecar txt file from video -> Created segmented clips with ffmpeg -> run a video-based caption classifier on clips -> semi-automate additional Danbooru/NovelAI format tags -> manual review dataset on Hydrus-> export to trainer
