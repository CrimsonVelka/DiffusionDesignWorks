icon:material/hammer-wrench
# Troubleshoot, Flaws, and Challenges

Despite everything I picked up and what I managed to accomplished on my own, there are still shortcomings due to time, in need of more experience, and inability to upscale my resources to continue gaining experience. 

## Model Quality
While I have impressed myself and friends with the quality of AI generation I was able to create that mimics that style I want, I have encountered some degrees of overfit and general stiffness with my generations. Most of these animations used in the current version are adapted from the works of a single main character designer, whose art style is being adapted by many different art directors across many studios, not just the ones used for this model. So it’s hard to tell where art style ends and overfit begins even when familiar with the the source material, but in time you can start to notice when certain features are creeping up when they are not wanted. This is why the next set of content ready for training will feature other ufotable adaptated works outside the current bubble its trained on to introduce new concepts but that still contain the consistent visual look by the studio. 
The old dataset has been getting replaced with my redone attempt; much cleaner, meticulously reviewed, and better frame data preservation that will hopefully improve the existing content already in the model with out negative change or unexpected changes. The dataset will continue to get larger and larger, with a focus to improve tags or concepts that struggle to get generated. The other improvement be pushing to train the model at higher and higher resolutions; 768x768 being the next goal, up to 1024 or even 1080x1080 if possible without breaking my machine. 

## Hardware
And to make sure I do not break my machine; I am working towards building an AI focused Home Lab with enough VRAM to run my stable diffusion training and can comfortably run and train LLMs as well. I did not realize early on that a 4090 was not enough to do many of the advance level AI activities. Even if I went to a double 3090 set up, it requires a specific motherboard configuration that supports SLI/NVLink because there is no reliable way to doing sharding for SD Model trainings without it, but not an issue when training LLMs. And while cloud-based training does exist, the amount of trial and error required to get things off the ground will add up very fast, that its basically cheaper to upfront all those costs on hardware.
Hard Drive space also became a big issue with the size of these datasets I was beginning to hoard. The Blu-Ray rips, having many versions of said Blu-Rays due to differences in fan encodes or the quality of the official discs that get pressed between different regions, also started eating into my hard drive space as they were also the source data for my datasets. I ended up purchasing 2 22TB hard drives and tried to find a space to shove them in my machine while I continue looking into RAID solutions.

## Tools
The challenge of building a pipeline is that you don’t know what you need until you ran into that problem. You can pseudocode and web chart out what you need at every step but until you start running that roadmap, you won’t know whether you took everything into account correctly. And when there was something, I wish could be automated, I found that no such tool was readily on hand. Many times I turned to ChatGPT to help me modify or even create these automatic scripts quickly to reach my end goal.
The Anime Screenshot Pipeline did give me insight on how I need to have everything set up, but not all of it’s tools were at the level I needed for my specific model, or simply didn’t work for my workflow. Mpdecimate wasn’t needed so I just used the standard ffmpeg command as is. There was a secondary computer vision application for similar image removal using FIftyOne, but because I already committed to the need of manual reviewing, is forewent its use and stuck with an image hash file based duplicate organizer. The Face Detect Auto Crop script in its form was not needed, so I removed the auto cropper and kept only face detect for subject tagging accuracy with a quick modification. I customized the use of Hydrus based on the new tools’ synergy with the image organizer instead of using a more mainstream dedicated application. 
And despite all of that, there are still some gaps in the pipeline that could probably use another automation tool, but it either doesn’t exist or requires a bit more technical knowledge beyond cheating with GPT to produce. It becomes a scale where I must balance taking the time to learn something that could save me some time in the future or spend that time doing it the way I already know because there is no guarantee that by the time I finish doing the process, that I would have found a solution. A shortcoming from working on this passion project on my own. 
While there are people that would like to help me, there is also time I would need to take to set up a working networking server that multiple people can sign into concurrently and do pieces of the work. Once again, take time away from training the work to get a server set up that maybe once in a while I will get a helping hand, or just continue working solo.

## Current State of Tech

AI is not perfect yet. Even when the big entities like StabilityAI produce a new version of Stable Diffusion 3, or Anlatan’s NovelAI v3, the changes and upgrades could result in an unexpected characteristic to appear that turns people away and that will wait for the field to continuing evolving so that a better version that feels like a direct improvement, rather than change for the sake of change, comes about. 
From what I have seen, I am the only one doing this kind of source material collecting, and further less at the level of “autism”, in which I am doing it. My idea is not original, the method to my madness to reach the quality that others with that idea strive to reach however, is unique because it’s made from passion. 
I’m working within the limits of the tech that is accessible to me. And the few crumbs of improvements and developments from other trainers doing similar levels of home-grown local finetuning, but for other themes or styles or end goals, I take in and get ready to apply for the next training. 
My dataset is not restricted to Stable Diffusion 1 or even Stable Diffusion all together, so whether tech becomes easier to access for ever aging hardware every generation leap, or I come into access of enterprise level resources to push the boundary, my work will never be wasted, it will be ready to adapt to the next best thing. And with that, I would like to segway into not necessarily proof, but a potential example of how a dataset meant for generative AI artwork, can be applied to different medium.
