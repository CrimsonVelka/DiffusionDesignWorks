{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Diffusion Design Works","text":"<p>Diffusion Design Works is a Stable Diffusion model series created with the goal of producing AI Art Generations with illustration styles mimicking the look and quality of studio produced Japanese animations.</p>"},{"location":"About/","title":"About Diffusion Design Works","text":"<p>Diffusion Design Works is a Stable Diffusion model series created with the goal of producing AI Art Generations with illustration styles mimicking the look and quality of studio produced Japanese animations.</p>"},{"location":"About/#how-is-this-different-from-other-loras","title":"How is this different from other LoRAs?","text":"<p>They are not a LoRA, but full checkpoints!</p> <p>LoRAs, while effective in introducing new concepts on an already trained main model in a non destructive way, can be inflexible if not trained properly. If running multiple different LoRAs, trained by different people of unknown skill or ability, will cause produce inconsistent final outputs in terms of quality and final content. Other \"full checkpoints\" that similarly go about recreating popular artists\u2019 and media\u2019s styles will train a LoRA based network on that content to then merge into an SD1.x, Novel AI, or SDXL base checkpoint, which is not normally allowed in basic SD model merge tools, through the use of something like SuperMerger. This merge is destructive, and will disrupt all the unet layers, making the final product produce different results than if the LoRA's were just injected during the generative process. </p> <p>By finetune new concepts and info from the very start, the model will have better flexibility in prompting, consistent results visual and quality results across all generations, and can be used with other extensions such as Controlnet or AnimateDiff that may struggle to incorporate generating content while additional networks are enabled.</p>"},{"location":"About/#text-to-image-models","title":"Text to Image Models","text":""},{"location":"About/#unlimited-diffusion-works","title":"Unlimited Diffusion Works","text":"<p>A television anime quality themed Stable Diffusion 1.4 based AI art model</p> <p> </p> <p>Trained in the likeness of production studio ufotable's works, the model is finetuned over Novel AI with 200K images. The generated art gives the feel and quality of official artwork or screenshots from a television series, most notably the adapted works of Type-Moon.</p> Img2Img Movie Poster Example + Inpaint time lapse <p> <pre><code>Steps: 30, Sampler: DPM++ 2M Karras, CFG scale: 8, Size: 608x896, Denoising strength: 0.6, Clip skip: 2\n</code></pre></p>"},{"location":"About/#text-to-video-models","title":"Text to Video Models","text":""},{"location":"About/#motion-model-diffusion","title":"Motion Model Diffusion","text":"<p>AnimateDiff Stable Diffusion Plug In to create Text to Video</p> <p> </p> <p>An indepent motion model finetune is possible, see page for information</p> <p>AnimateDiff is a seperate motion video model that integrates the unet block information of the loaded model to generate the individual frames that then get joined together into an animation.</p> Animation + Contact Sheet of frames <p> </p>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/posts/testing/","title":"Hello world!","text":"<p>Hello world mother fuckers</p>"},{"location":"pages/faq/lessonslearned/","title":"What I have learned for undertaking this project","text":"<p>All I wanted to do was just make AI art in the style of my favorite shows. Just wanted to cook something up in-between my odd jobs and video game sessions. What I got instead was a journey through several different fields of work, disciplines within art photography and animation, and the roughness of the math and science behind what we all call AI. </p> <ul> <li>I learn a lot of python and CUDA, sometimes had to cheat with ChatGPT but was able to recognize how to get things working.</li> <li>I learned a lot about Blu-Ray content. The encoding process of fanrips, streaming services, and the physical delivery of the media all playing a part in the quality of the viewing experience and how that was affecting tools I used when curating my dataset from these sources.</li> <li>Getting a refresher on various art concepts and terminology so I could better recognize concepts when tagging my dataset.</li> <li>And while doing all of this dataset creation and compiling, I learned a lot about the animation creation process, from genga drawing to digital coloring to CGI set pieces to the final finish that goes on the silver screen or your living room television.</li> </ul> <p>I think I learned more about all these other fields and discipline than I did the back end of the AI generative creation process, but I can now easily recognize how to take what I have learned and translate it to working on LLMs for textgen models or maybe even audiogen models. And as I continue to grow this model, and improve my access to hardware, I hope to continuing learn more and creating more with AI.</p>"},{"location":"pages/motion/localtraining/","title":"Local Training of AnimeateDiff","text":""},{"location":"pages/motion/localtraining/#homebrew-potential","title":"Homebrew Potential","text":"<p>Sometime this year, the developers of AnimateDiff disclosed their training method, and since then only one decent finetune fork was created where the training resolution was doubled, but no new data was added due a lack of video tagging tools. While not a significant leap, it shows that enthusiasts are interested in pursuing this tech. With the visibility of the video dataset now clear, the path is open for those that have the hardware to finetune the motion model. </p> <p>WebVid10M (Mirror for education purposes), is comprised of low-resolution watermarked previews of shutterstock footage run through their own classifier as part of a larger project the group worked on. With a good stating off point, my existing pipeline for SD can be adjusted to incorporate new video clips, need only to use their Video trainer <code>Video2dataset</code> and repurpose the media rips I already have.</p>"},{"location":"pages/motion/localtraining/#theoretical-pipeline","title":"Theoretical Pipeline","text":"<p>Since specific tuning is not required, we are open to using more types of movies, recordings, and other animations not necessarily related to official films and shows or even the ones we are using. </p> <p>Say we have access to a collection of videos. What we can create a script that will generate timestamps, similar to chapter select codes for Blu-Rays or steaming services, to mark every time there is a jumpcut to a new scene or camera shot. This will get written onto a sidecar file.  Then with ffmpeg, use the video clip command with the sidecar automate the video clips slices. These clips will then be tagged and captioned and can be organized within Hydrus (Hydrus also can organize video format files) and use the same image organization tools to manually review tags and captions every time we need to train the model. </p> <p>Potential Prompting Improvement</p> <ul> <li>An addition step that can be performed to improve prompting results on the finetune is to introduce the Danbooru/Novel AI tag format as well as keeping the natural language captioning. Keeping the tagging consistent across both the main model and the motion model will cause less confusion for the final generation as both models will have a similar format to follow. </li> <li>A method to do this would be to take some frames from the peak of each clip, run the SD Tagger on it, and then apply those tags onto its corresponding video clip on Hydrus. </li> </ul> <p>Steps:</p> <ol> <li>FFMPEG <ul> <li>Create scene jump cut timestamp sidecar txt file from video </li> <li>Created segmented clips with ffmpeg </li> </ul> </li> <li>Tagger (Video)<ul> <li>Run a video-based caption classifier on clips </li> </ul> </li> <li>Tagger (Image - Optional)<ul> <li>Semi-automate additional Danbooru/NovelAI format tags <ul> <li>Use frames from scenes to aggregate and average out compatible tags</li> </ul> </li> </ul> </li> <li>Hydrus<ul> <li>Manual review dataset</li> </ul> </li> <li>Video2dataset<ul> <li>Export to trainer</li> </ul> </li> <li>Debug<ul> <li>Rinse and Repeat</li> </ul> </li> </ol> <p>From there it would be business as usual on the training front, depending on whether our local resources are sufficent to run the job ourselves.</p>"},{"location":"pages/motion/motiondiffusion/","title":"Local Video Motion Model","text":"<p>As Text to Video has taken its first steps into mainstream with products such as Luma, interest in making this kind of AI content has grown, and there have been some groups that have made attempts in making video models for local applications in the open source space that integrate with Stable Diffusion.  </p>"},{"location":"pages/motion/motiondiffusion/#animatediff","title":"AnimateDiff","text":"<p>AnimateDiff is a plug-and-play module that will take T2I prompts and use the motion model to create short animation video clips with Stable Diffusion. This was created back in June 2023.</p> <p>One of the issues as to why this never picked up much steam, aside from its incompatibility with SDXL at the time, is that there was no clear way or instruction to make the video generations do what you want. Had to bruteforce your way into a result and sometimes you just settled for what you got. The hardware requirements were also much higher, most people could only make 256x256 to about 384x384 clips and the generations would take 10 to 15 minutes to generate. They also could not be upscaled without running out of memory. </p>"},{"location":"pages/motion/motiondiffusion/#animatediff-udw-examples","title":"AnimateDiff + UDW Examples","text":"<p>Reaniamted with Flowframes - RIFE 4.0 @ 20 FPS</p> <p>While fortunate enough to have a 4090 and able to generate 768x512 clips, they still took a significant amount of time to train and were still prone to failing mid generation if there was some sort of hiccup for the bleeding edge tech at the time. There is some optimizations since then, more GPU VRAM is require to do higher resolutions, hires upscales, and longer generations.</p> <p>.gif outputs</p> <p>One other drawback to AnimateDiff is that outputs are generated in a very low bit gif format and around 8FPS by default, but you have access to all your generated frames at their maximum generated quality, so you could reanimate them in third party software or output into a better file container like mp4 or webm at higher quality clip at a better frame rate. </p>"},{"location":"pages/motion/motiondiffusion/#competition","title":"Competition","text":""},{"location":"pages/motion/motiondiffusion/#luma","title":"Luma","text":"<p>Services like Luma do have a better quality to their video generations, even when generating off an image for Img2Vid. While short in length, they are still longer than animatediff and maintain better quality by default as far as the platform is doing additional work in background. </p>"},{"location":"pages/motion/motiondiffusion/#img2vid-examples","title":"Img2Vid Examples","text":"<p>Starting Source Images</p> <p></p> <p>Generated Results for Img2Vid</p> <p>But being restricted to their base model and their training may produce unintended results. Using their enhance prompt feature will deviate from your image almost entirely if your image is based on something their model is not trained on. If you focus exclusively on Text to Video, its not a bad platform, but if you want more control over the content you want to create with what you have, we would have to look else where.</p> Example of non-consistent Img2Vid <p></p> <p> </p>"},{"location":"pages/motion/news/","title":"News","text":"<p>In addition to ufotable, I added for experimentation purposes content from the following non-ufotable works for additional data and regularization purposes:</p> <p>Non-themed sources</p> <ul> <li>Lord El-Melloi II Case Files (studio TROYCA)</li> <li>additional miscellaneous non-animation data</li> </ul>"},{"location":"pages/udw/datasetpostprocess/","title":"Dataset Post-Processing","text":""},{"location":"pages/udw/datasetpostprocess/#overview","title":"Overview","text":"<p>There are 3 additional processes I do to my dataset that attempt to fix certain flaws in the dataset, cleaning up data that could have production mistakes or bothersome editing choices, and maximize the amount of detail of specific shots or scenes: Layering, Cropping, and Stitching.</p> <p>Things to note before continuing:</p> <ul> <li> <p>SD1.x is trained on 512x512 resolution images, a size of 262,144 Pixels</p> </li> <li> <p>Novel AI continues training from SD1.4 with 768x768 resolution images, a size of 589,824 Pixels</p> </li> <li> <p>SDXL is trained on 1024x1024 resolution images, a size of 1,048,576 Pixels</p> </li> <li> <p>Novel AI created Aspect Ratio Bucketing to allow training images to be outside 1:1 aspect resolutions, but the image is resized to fit within the 1:1 training resolution's pixel count</p> </li> <li> <p>Most enthusiasts\u2019 LoRAs are still trained on 512x512 due to the averae person's hardware constraints but have Aspect Ratio Bucketing incorporated for varied training resolution sizes.</p> </li> </ul>"},{"location":"pages/udw/datasetpostprocess/#layering","title":"Layering","text":"<p>Example scene from Garden of Sinners 7</p> <p>So I have this scene of store front backdrop with a lot of moving foreground pieces, but not a single frame has a clean shot of that backdrop. There appears to be enough frames to piece together separate parts of the images on top of each other, and mask out unwanted details, to create a clean backdrop image.  </p> <p></p> <p>Layering the scene</p> <p>We start with a base frame that has the least amount of unwanted foreground pieces, then takes pieces of other frames that have the most negative space that show the store front, and start layering them on top of eachother until we get the clean backdrop. From there we reincorporate the base frame with a crop of the two main subjects into center focus so we can produce a backdrop frame with the subject for additional concept training.</p> <p> </p> <p>Final Results</p> <p>Then the original frames are reintroduced into the dataset with the frames created to train the concept of being able to draw a location, and that same spot with moving crowds or objects in the foreground. This is not something I prioritize often, but when a particular scene with too many moving pieces shows up that I feel is worth doing the work produce, I will set that scene aside to make an attempt later.</p>"},{"location":"pages/udw/datasetpostprocess/#stitching","title":"Stitching","text":"<p>Example scenes from Fate Zero, Unlimited Blade Works, and Heaven's Feel</p> <p>In any sort of motion picture, you will get scenes where the camera is panning from one direction another to either show off a landscape shot, or a character\u2019s reveal a sizing up of several characters in the prelude to a confrontation. Most of these frames by themselves wouldn't be useful, but merging them to create a full image would give better concept training than a mass of images the model would have no way to assemble on its own, let alone any understanding of what it's looking at.</p> <p> </p> <p>Frames stitched up into full images</p> <p>Simply importing the frames to Photoshop or Lightroom and use either manual or automated tools to assemble the pieces together will create a good quality image. The full image in non-bluray aspect ratios will also give the model additional resolutions to train on to improve generations outside the standard sresolutions and reduce potential overfit that can be caused by training almost exclusively on 16:9 resolution images.</p> <p>Parallaxing</p> <p>There may be issue with stitching up images due to parallaxing between foreground, background, and other objects. So not only will there be cases where one must manually make adjustments when automation fails, but also judge which set pieces need to be the anchor points to line up the other images and mask out incompatible details similar to how the layering example is done.</p>"},{"location":"pages/udw/datasetpostprocess/#cropping","title":"Cropping","text":"<p>Example scenes from Fate Heaven's Feel 2</p> <p>While originally a requirement to make 1:1 crop cutouts of images for your dataset in the early days of Stable Diffusion Dreambooth training, the introduction and source code release for Aspect Ratio Bucketing by NovelAI allowed the use of any resolution size for datasets and would be sized to fit a resolution bucket for training. </p> <p>This crop and resizing down of the full images however will cause a loss in detail of the image as the bucket still needs to comply with the 1:1 aspect pixel size, and those buckets will always have a lower pixel count than the default training resolution, so a large image like a 1920x1080 blu-ray frame or any other 2MP image from an imageboard getting crunched down will cause the model to lose out on information detail. </p> <p></p> <p>Expressions made into individual cropped images</p> <p>To remedy this, I also include sizable amount of 1:1 aspect images. These focus on facial expressions, certain detail focus shots of objects and items, character\u2019s extremities to improve hand and finger accuracy, or close ups of clothing and/or patterns. I will also use this on the same frames of stitches I created, if possible, to also preserve those details of panning shots. The 1:1 aspect images also help in reducing overfit from the main resolution. These 1:1 crops are almost always 1080x, so they were made with future training at SDXL resolution (or 1080x1080!) in mind.</p>"},{"location":"pages/udw/flaws/","title":"Troubleshoot, Flaws, and Challenges","text":"<p>Despite everything I picked up and what I managed to accomplished on my own, there are still shortcomings due to time, in need of more experience, and inability to upscale my resources to continue gaining experience. </p>"},{"location":"pages/udw/flaws/#model-quality","title":"Model Quality","text":"<p>While I have impressed myself and friends with the quality of AI generation I was able to create that mimics that style I want, I have encountered some degrees of overfit and general stiffness with my generations. Most of these animations used in the current version are adapted from the works of a single main character designer, whose art style is being adapted by many different art directors across many studios, not just the ones used for this model. So it\u2019s hard to tell where art style ends and overfit begins even when familiar with the the source material, but in time you can start to notice when certain features are creeping up when they are not wanted. This is why the next set of content ready for training will feature other ufotable adaptated works outside the current bubble its trained on to introduce new concepts but that still contain the consistent visual look by the studio.  The old dataset has been getting replaced with my redone attempt; much cleaner, meticulously reviewed, and better frame data preservation that will hopefully improve the existing content already in the model with out negative change or unexpected changes. The dataset will continue to get larger and larger, with a focus to improve tags or concepts that struggle to get generated. The other improvement be pushing to train the model at higher and higher resolutions; 768x768 being the next goal, up to 1024 or even 1080x1080 if possible without breaking my machine. </p>"},{"location":"pages/udw/flaws/#hardware","title":"Hardware","text":"<p>And to make sure I do not break my machine; I am working towards building an AI focused Home Lab with enough VRAM to run my stable diffusion training and can comfortably run and train LLMs as well. I did not realize early on that a 4090 was not enough to do many of the advance level AI activities. Even if I went to a double 3090 set up, it requires a specific motherboard configuration that supports SLI/NVLink because there is no reliable way to doing sharding for SD Model trainings without it, but not an issue when training LLMs. And while cloud-based training does exist, the amount of trial and error required to get things off the ground will add up very fast, that its basically cheaper to upfront all those costs on hardware. Hard Drive space also became a big issue with the size of these datasets I was beginning to hoard. The Blu-Ray rips, having many versions of said Blu-Rays due to differences in fan encodes or the quality of the official discs that get pressed between different regions, also started eating into my hard drive space as they were also the source data for my datasets. I ended up purchasing 2 22TB hard drives and tried to find a space to shove them in my machine while I continue looking into RAID solutions.</p>"},{"location":"pages/udw/flaws/#tools","title":"Tools","text":"<p>The challenge of building a pipeline is that you don\u2019t know what you need until you ran into that problem. You can pseudocode and web chart out what you need at every step but until you start running that roadmap, you won\u2019t know whether you took everything into account correctly. And when there was something, I wish could be automated, I found that no such tool was readily on hand. Many times I turned to ChatGPT to help me modify or even create these automatic scripts quickly to reach my end goal. The Anime Screenshot Pipeline did give me insight on how I need to have everything set up, but not all of it\u2019s tools were at the level I needed for my specific model, or simply didn\u2019t work for my workflow. Mpdecimate wasn\u2019t needed so I just used the standard ffmpeg command as is. There was a secondary computer vision application for similar image removal using FIftyOne, but because I already committed to the need of manual reviewing, is forewent its use and stuck with an image hash file based duplicate organizer. The Face Detect Auto Crop script in its form was not needed, so I removed the auto cropper and kept only face detect for subject tagging accuracy with a quick modification. I customized the use of Hydrus based on the new tools\u2019 synergy with the image organizer instead of using a more mainstream dedicated application.  And despite all of that, there are still some gaps in the pipeline that could probably use another automation tool, but it either doesn\u2019t exist or requires a bit more technical knowledge beyond cheating with GPT to produce. It becomes a scale where I must balance taking the time to learn something that could save me some time in the future or spend that time doing it the way I already know because there is no guarantee that by the time I finish doing the process, that I would have found a solution. A shortcoming from working on this passion project on my own.  While there are people that would like to help me, there is also time I would need to take to set up a working networking server that multiple people can sign into concurrently and do pieces of the work. Once again, take time away from training the work to get a server set up that maybe once in a while I will get a helping hand, or just continue working solo.</p>"},{"location":"pages/udw/flaws/#current-state-of-tech","title":"Current State of Tech","text":"<p>AI is not perfect yet. Even when the big entities like StabilityAI produce a new version of Stable Diffusion 3, or Anlatan\u2019s NovelAI v3, the changes and upgrades could result in an unexpected characteristic to appear that turns people away and that will wait for the field to continuing evolving so that a better version that feels like a direct improvement, rather than change for the sake of change, comes about.  From what I have seen, I am the only one doing this kind of source material collecting, and further less at the level of \u201cautism\u201d, in which I am doing it. My idea is not original, the method to my madness to reach the quality that others with that idea strive to reach however, is unique because it\u2019s made from passion.  I\u2019m working within the limits of the tech that is accessible to me. And the few crumbs of improvements and developments from other trainers doing similar levels of home-grown local finetuning, but for other themes or styles or end goals, I take in and get ready to apply for the next training.  My dataset is not restricted to Stable Diffusion 1 or even Stable Diffusion all together, so whether tech becomes easier to access for ever aging hardware every generation leap, or I come into access of enterprise level resources to push the boundary, my work will never be wasted, it will be ready to adapt to the next best thing. And with that, I would like to segway into not necessarily proof, but a potential example of how a dataset meant for generative AI artwork, can be applied to different medium.</p>"},{"location":"pages/udw/todolist/","title":"Status Update + To Do List","text":""},{"location":"pages/udw/todolist/#082024","title":"08/2024","text":""},{"location":"pages/udw/todolist/#content-update","title":"Content Update","text":""},{"location":"pages/udw/todolist/#tales-of-series-81604","title":"Tales of Series - (8/16/04)","text":"<p>Frames from Tales of Arise Opening #1</p> <p>The following content will be included in the next training session - minimal manual edits only:</p> <ul> <li>Video Games - Opening Animations and Cutscenes only <ul> <li>Tales of Xillia I</li> <li>Tales of Xillia II</li> <li>Tales of Arise      </li> </ul> </li> </ul> <p>Someone finally took the time to decrypt Tales of Arise's USM key, so I was able to use it to demux the files I had on stand by and took the opportunity to work on extracting Arise plus organize Xillia 1 &amp; 2 to add onto the next training. Won't do a big clean, just a quick dedup and yank out any bad looking frames just to see how more of Akira Matsushima (Chief Animation Direction and Animation Character Designer for Xillia, Zestiria The Cross, and Arise) would change up some lacking blind spots in the data.</p> <p>While in total its around 115k more frames between the 3 game's animations, realistically maybe only 30k will make the final cut in the final model.</p>"},{"location":"pages/udw/todolist/#fate-stay-night-realta-nua-20th-anniversary-remastered-update","title":"Fate Stay Night + [Realta Nua] 20th Anniversary Remastered Update","text":"<p>Found out that they included the 1080p versions of the opening cutscenes that were only in 960x544 quality on the Playstation Vita version of the release so I now have those fully extracted and deleted the old Vita ones I dumped from the ROM last year. And after a quick look at the FGO stuff, I'm gonna hold off on adding the ufotable commercial promos and just include the in-game art they drew. The commercials are just YouTube rips so I'm at the mercy of YT's encoding being decent at higher quality... its not.</p>"},{"location":"pages/udw/todolist/#ufotable-key-animation-collections","title":"ufotable Key Animation Collections","text":"<p>Fate/Stay Night 'Unlimited Blade Works' Character Complete Key Animations 1st - 2nd Season Comiket C88 Banner</p> <p>The following content to be added to a future training:</p> <ul> <li>Artbook<ul> <li>ufotable <ul> <li>Fate/Stay Night 'Unlimited Blade Works' Character Complete Key Animations #00 \u00b7 #01<ul> <li>Saber - Sample</li> <li>Archer - Sample - Illustration Card</li> <li>Toksaka Rin - Sample - Illustration Card </li> </ul> </li> <li>Fate/Stay Night 'Unlimited Blade Works' Character Complete Key Animations 1st - 2nd Season<ul> <li>Lancer - Sample</li> <li>Gilgamesh - Sample</li> </ul> </li> </ul> </li> </ul> </li> </ul> <p>I ironically mentioned in a previous update that these Key Animations were likely going to be low priority in acquiring because there are too many of them, very expensive to obtain, and no cases I was aware of where anyone ever scanned their collection to show off these rare box sets... until about a couple days ago as of this writing. Someone scanned several of these animation key frames books. Unfortunately they were scanned in black and white, they should actually be partially colored to denote color separation and shading. The scans also did not contain the illustration cards, but I had some scans of them already. Some of the frames look like they came from another collection sets, but still match the theme so I will still add them as they are absurdly high resolution with clear detailing, until I find the colored versions.</p>"},{"location":"pages/udw/todolist/#dataset-experiment","title":"Dataset Experiment","text":"<p>I'm also preparing a different kind of dataset sample from a Visual Novel that has almost fully animated sprites and cutscenes but are already seperated in frames. The game's engine would assemble all of the keyframes together in real time, so no need to frame extract the content. I would have to reassemble and animate the the scenes if I want to add it for the motion model's dataset when I attempt the finetune once UDW's next training is completed.</p> <p>Will also make a point to try and get some Visual Novel assets from Witch on the Holy Night and Tsukihime prepared for this next training. Introduce the other main Type Moon artist's (Hirokazu Koyama) character designs that will be adapted in a new movie with the former, and reinforce the modern Takeuchi character designs already adapted by ufotable with the latter. The goal is to improve clean sprite resolution depictions of characters and that can be blown up to larger resolutions, using their ultra high resolution sources images (1200x5400 for example) that can be resized down or chopped up to fit various form of image composition sizes and have data for up close detailing.</p>"},{"location":"pages/udw/todolist/#072024","title":"07/2024","text":""},{"location":"pages/udw/todolist/#content-updates","title":"Content Updates","text":""},{"location":"pages/udw/todolist/#manual-review-and-clean-up","title":"Manual Review and Clean Up","text":"<p>The following content in the current model is undergoing quality control and clean up:</p> <ul> <li>Fate Zero<ul> <li>Episode 1-19 re-extracted, Episodes 1-3 and OP/ED 1 and 2 Completed, manual review and post work still in progress</li> </ul> </li> <li>Fate Stay Night Unlimited Blade Works (TV)<ul> <li>Episode 0-12 re-extracted, Episode 0-2 and OP/ED 1 and 2 Completed, manual review and post work still in progress</li> </ul> </li> <li>Fate Stay Night Heaven\u2019s Feel<ul> <li>1 \u2013 presage flower: re-extracted, deduped and sorted, undergoing post-processing. manual review need</li> <li>2 \u2013 lost butterfly: re-extracted, deduped and sorted, undergoing post-processing. manual review need</li> <li>3 \u2013 spring song: re-extracted, duplicate removal and sorting in progress, some stitching completed</li> </ul> </li> <li>The Garden of Sinners <ul> <li>8 \u2013 Epilogue: re-extracted, deduped and sorted, undergoing post-processing, manual review done</li> <li>9 \u2013 Future Gospel: re-extracted, clean up in Progress, some stitching completed</li> <li>9.5 Future Gospel \u2013 Extra Chorus: re-extracted, deduped and sorted, undergoing post-processing, manual review done</li> </ul> </li> <li>Fate Stay Night [R\u00e9alta Nua]<ul> <li>Opening Animations<ul> <li>re-extracted, deduped and sorted, undergoing post-processing. manual review need</li> </ul> </li> </ul> </li> <li>Fate/Hollow Ataraxia<ul> <li>Opening Animations<ul> <li>re-extracted, deduped and sorted, undergoing post-processing. manual review need</li> </ul> </li> </ul> </li> </ul>"},{"location":"pages/udw/todolist/#ready-to-train","title":"Ready To Train","text":"<p>The following content will be included in the next training session - minimal manual edits only:</p> <ul> <li>Movies<ul> <li>The Garden of Sinners <ul> <li>3 \u2013 Remaining Sense of Pain</li> <li>4 \u2013 The Hollow Shrine</li> <li>5 \u2013 Paradox Spiral</li> <li>6 \u2013 Oblivion Recorder</li> <li>7 \u2013 A Study in Murder - Part 2</li> </ul> </li> </ul> </li> <li>TV <ul> <li>Katsugeki/Touken Ranbu</li> <li>Tales of Zestiria the X  (Season 1)</li> </ul> </li> <li>Video Games<ul> <li>Tales of Zestiria<ul> <li>Opening Animations and Cutscenes</li> </ul> </li> <li>Tales of Berseria<ul> <li>Opening Animations and Cutscenes</li> </ul> </li> <li>Tsukihime -A piece of blue glass moon-<ul> <li>Opening Animations</li> </ul> </li> </ul> </li> <li>Mobile<ul> <li>Fate Grand Order<ul> <li>[The Garden of Sinners] crossover event<ul> <li>ufotable assets and promotional animation only</li> </ul> </li> <li>[Fate Zero] crossover event<ul> <li>ufotable promotional animation only</li> </ul> </li> <li>[Fate Stay Night - Heaven's Feel] movie premiere promotions<ul> <li>ufotable assets only</li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"pages/udw/todolist/#in-progress","title":"In Progress","text":"<p>The following will be included in a future session - not ready, not ufotable content, lower priority, or limited disk space</p> <ul> <li> <p>Artbooks</p> <ul> <li>The Garden of Sinners: The Animation by Ufotable<ul> <li>Have physical source, needs to be scanned</li> </ul> </li> <li>The Garden of Sinners: Complete Illustration Art Book by Takeuchi Takashi<ul> <li>Have physical source, needs to be scanned</li> </ul> </li> </ul> </li> <li> <p>Movies</p> <ul> <li>The Garden of Sinners <ul> <li>1 \u2013 Overlooking View<ul> <li>Have source media, not extracted</li> </ul> </li> <li>2 \u2013 A Study in Murder - Part 1: <ul> <li>Have source media, not extracted</li> </ul> </li> </ul> </li> <li>Tales of Symphonia<ul> <li>Have not finalized a source</li> </ul> </li> <li>Demon Slayer - Mugen Train<ul> <li>Have not finalized a source</li> </ul> </li> </ul> </li> <li>TV<ul> <li>Tales of Zestiria the X (TV - Season 2)<ul> <li>Have source media, not extracted</li> </ul> </li> <li>Today's Menu for the Emiya Family  <ul> <li>Have source media, not extracted </li> </ul> </li> <li>Demon Slayer<ul> <li>Have not finalized a source</li> </ul> </li> </ul> </li> <li>Video Games<ul> <li>Tales of Xillia I<ul> <li>Openings and cutscenes extracted, Not sorted</li> </ul> </li> <li>Tales of Xillia II<ul> <li>Openings and cutscenes extracted, Not sorted</li> </ul> </li> <li>Witch on the Holy Night<ul> <li>Not ufotable, assembling in-game sprites and CG assets</li> </ul> </li> <li>Tsukihime -A piece of blue glass moon-<ul> <li>Assembling non-ufotable in-game sprites and CG assets</li> </ul> </li> </ul> </li> <li>Mobile<ul> <li>Tales of Asteria <ul> <li>ufotable assets only</li> </ul> </li> <li>Tales of Crestoria<ul> <li>ufotable assets only</li> </ul> </li> </ul> </li> </ul>"},{"location":"pages/udw/todolist/#tbd","title":"TBD","text":"<p>Awaiting sale or alternate source for follow collections</p> <ul> <li>Artbook<ul> <li>ufotable <ul> <li>The Garden of Sinners - Seikaisha (Background Art Book)</li> <li>The Garden of Sinners - Future Gospel New Years Illustrations Postcards [Comiket 85]<ul> <li>A scan of this exists online but what looks to be dust or fuzz on the images from when it was scanned is obscuring fine details.</li> </ul> </li> <li>Fate/Zero - Seikaisha (Background Art Book)</li> <li>Fate/Stay night 'Unlimited Blade Works' - Seikaisha (Background Art Book)</li> <li>Fate/Stay Night 'Unlimited Blade Works' S1 - Settei Shiryoushuu (Setting Material) [Comiket 87]</li> <li>Fate/Stay Night 'Unlimited Blade Works' S2- Settei Shiryoushuu (Setting Material) [Comiket 88]</li> <li>Fate/Stay Night 'Unlimited Blade Works' Character Complete Key Animations [Comiket 87, 88, 89]<ul> <li>There's 10 of these, very low priority in seeking out product or scans of it other than to obtain it's included lineart image.</li> </ul> </li> </ul> </li> </ul> </li> </ul> <p>Waiting on releases for the following Ufotable Media:</p> <ul> <li>Movies<ul> <li>Witch on the Holy Night<ul> <li>Have rips of the trailers for quality source testing on standby: #1, #2</li> </ul> </li> </ul> </li> <li>TV<ul> <li>Untitled Genshin Impact Animation<ul> <li>Have rip of trailer for quality source testing on standby</li> </ul> </li> </ul> </li> <li>Mobile<ul> <li>Honkai Starrail <ul> <li>[Fate Stay Night - Unlimited Blade Works] crossover event</li> </ul> </li> </ul> </li> </ul>"},{"location":"pages/udw/udwmodel/","title":"Unlimited Diffusion Works (UDW)","text":""},{"location":"pages/udw/udwmodel/#overview","title":"Overview","text":"<p>Unlimited Diffusion Works is a model based on the works of animation studio ufotable, whose works include:</p> <ul> <li>14 Animated Films</li> <li>15 TV Shows </li> <li>17 Video Game Opening Animations and Cutscenes</li> </ul> <p>This model will focus on their studio's adaptations of works by Type-Moon, and will include their collaborations on series produced by Namco Bandai, to have a set level of character design, color grading, quality consistency.</p>"},{"location":"pages/udw/udwmodel/#dataset","title":"Dataset","text":"<p>This model\u2019s current version was trained on 09/2023, with around 200k images finetuned over NovelAI v1, uses the following content from studio ufotable:</p>"},{"location":"pages/udw/udwmodel/#media","title":"Media","text":"<p>ufotable sources</p> <ul> <li>Fate Zero </li> <li>The Garden of Sinners<ul> <li>8 - Epilogue</li> <li>9 - Future Gospel</li> <li>9.5 - Future Gospel: Extra Chorus</li> </ul> </li> <li>Fate Stay Night [R\u00e9alta Nua]</li> <li>Fate Stay Night Unlimited Blade Works</li> <li>Fate/Hollow Ataraxia</li> <li>Fate Stay Night Heaven\u2019s Feel<ul> <li>1 - presage flower</li> <li>2 - lost butterfly</li> <li>3 - spring song</li> </ul> </li> </ul> <p>The original dataset was created by running Blu-ray rips of the listed content through ffmpeg as part of the aid of Anime Screenshot Pipeline on Github back in February of 2023. But I have since used my own workflow pipeline processes and modified or used new tools to improve the training and creation of the current model back in 09/2023.</p>"},{"location":"pages/udw/udwmodel/#media-source","title":"Media Source","text":"<p>I chose Blu-Rays rather than WEBRips or broadcast recordings because the bitrate of the transmition plus recording software's encoding methods harm the visual quality of the content and those imperfections will be trained onto the model. Demuxing the BluRay ensures that I have the highest quality source for the frames I will be extracting. It's also worth noting not every Blu-Ray release is good, and sometimes a different region's BluRay release (ITA and GER notably) may have a cleaned up and better encoded file made by that publisher pressed onto their discs to produce a much better visual than the original JP or ENG (which is sometimes just the same JP) Blurays but can take months or even a full year after the first regions have been released and sold. </p> <p>I have used a mixture of different regional Blu-Ray discs based on what other conesours have considered to be the best version available for each of the above series.</p>"},{"location":"pages/udw/udwmodel/#sample-gallery","title":"Sample Gallery","text":""},{"location":"pages/udw/udwmodel/#main-theme-assorted-samples","title":"Main Theme Assorted Samples","text":""},{"location":"pages/udw/udwmodel/#169-screen-resolution-tvmovie-look-examples","title":"16:9 Screen Resolution TV/Movie Look Examples","text":""},{"location":"pages/udw/udwmodel/#key-visual-resolution-style-examples","title":"Key Visual Resolution Style Examples","text":""},{"location":"pages/udw/udwmodel/#concept-art-style-mock-up-examples","title":"Concept Art Style Mock Up Examples","text":""},{"location":"pages/udw/udwpipeline/","title":"Pipeline Workflow","text":""},{"location":"pages/udw/udwpipeline/#evolving-from-a-preset-repo","title":"Evolving from a Preset Repo","text":"<p>After several successful model trainings, I had become dissatisfied with the quality of the dataset I originally produced with Anime Screenshot Pipeline. It had some short commings with the quality of data and some steps and scripting were unnecessary for what I needed. Through repurposing and editing some of tools and scripts, I began redoing the entire dataset from once I got comfortable with the entire training process. By the time I reached my current workflow and results, my pipeline workflow looks unrecognizable to even the developer's current version of the process. There is some more manual steps involved but not from lack of ability, but rather the need of human supervision in so places where automation is not yet up to the task.</p> <p>Things to note before continuing:</p> <ul> <li> <p>Anime is usually broadcasted in 24 frames per second, drawn in 3s. Meaning every drawing is on screen for 3 frames, 8 different frames shown in a second.</p> </li> <li> <p>A standard anime episode is 24 minutes long, or around 34k frames</p> </li> <li> <p>A standard film of 2 hours is around 170k frames</p> </li> <li> <p>Each frame when exported through ffmpeg is around 1.5MB-3MBs in size</p> </li> <li> <p>This makes the size of frame extractions 300GBs for a single episode and 1.5TB for a movie. </p> </li> <li> <p>A manually cleaned up episode will come down to about 50GBs in size and a movie will drop to around 600GB, but takes about a days worth of work.</p> </li> <li> <p>Very large Hard Drive space required</p> </li> </ul>"},{"location":"pages/udw/udwpipeline/#frame-extraction","title":"Frame Extraction","text":"<p>The first change I made to the pipeline was the <code>ffmpeg</code> script provided. The Pipeline used a script that would run the mpdecimate command to remove duplicate or \u201cfrozen frames\u201d, moments in the animation where there is no movement, as <code>ffmpeg</code> is \u201cextracting\u201d the frames to condense down the frames to a final output around one tenth of the original size. Then a secondary filter using a computer vision model through a Jupyter script using <code>FiftyOne</code> would do one last sweep of any frozen frames mpdecimate would've missed to reduce the file size even further.</p> <p></p> <p>A selection of frames from 6 different \"Fate/Stay Night\" scenes extracted through ffmpeg.</p> <p>However, I came to find out much later there was a large spread of false positive frame removals for unknown reasons, and false negatives of frames that were kept from a combination of mpdecimate not considering jump cuts that repeat scenes after several cuts, and FiftyOne failing the second filter due to the Blu-ray encodings introducing enough pixelation changes between frames that it would defeat both filters' deletion threshold. </p> <pre><code>ffmpeg -hwaccel cuda -i \"video.m2ts\" \"Name_EpisodeNumber\"_%d.png\n</code></pre> <p>Because the loss of data was too significant to ignore, I stopped using mpdecimate and let ffmpeg run the simple command above to extract every single frame in full. Then instead of using the FiftyOne's CV, I began using a script by space-nuko that would sort the \"forzen frames\" based on the image file's hash value and would only move all the flagged frames to a different folder and not delete the rest of the images, allowing for manual review in the future. </p> <pre><code>python tagtools.py -r dedup \"\\path\\to\\folder\"\n</code></pre> <p></p> <p>Unlimited Blade Works Episode 3 scenes sorted into different folders. 384 total unique jump cuts, 340 of those folders will be sorted and reviewed.</p> <p>The downside to this new method is that increased use of hard drive space from not just more images, but the file size was doubled or tripled in some cases. Mpdecimate's outputs were shrinking the .png file size to under 1MB, and ffmpeg on it's own does not have settings to reduce file sizes without destroying the pixelation of the output. </p> <p>This situation would force me to start buying hard drives and to looking into NAS solutions to hold all this data, but it did also present a way to schedule when to start manual reviewing the datasets, as this would free almost enough space to frame extract another episode and give some breathing room before going out to buy more storage.</p>"},{"location":"pages/udw/udwpipeline/#tagging","title":"Tagging","text":"<p>Stable Diffusion base models use CLIP captioning to tag their datasets with natural language for prompt keywords. NovelAI\u2019s anime finetune of SD1.4 instead used English based Japanese imageboard site Danbooru\u2019s key word tagging format for prompting instruction. This is because Novel AI's parent company, Anlatan, scrapped Danbooru's site for all the images used in the training of their model, and used their metadata as the standard for prompting their model. Because it seemed that keyword prompting performed better than just pure natural language, and the tool creators made popular tools for anime trainers based on NovelAI's content, future dreambooths, LoRA networks, and other checkpoints types followed this format for not just anime, but realism models as well.</p> <p></p> <p>Automatic1111's Tagger extension menu displaying \"Batch from Directory\" mode</p> <p>I initially used the built in Automatic1111 tagger when starting out making Textual Inversions and LoRAs and worked fine for the standard images you would find on the internet. It will generate a sidecar text file that will be read by the trainer to associate the metadata with the image. The problem comes when working on 16:9 images, as all the classifiers out in the wild were trained mostly with either 1:1 aspect images due to SD1.x early on requiring 512x512 data, or whatever sizable collection of images trainer could collect in other resolution sizes for regularization. </p> Examples of how the mistagging problem manifested in a 02/2023 finetune attempt: <p>Yea I don't think these were normal...</p> <p> </p> <p>The 16:9 ratio confused the classifiers and produced a high amount of false positives tagging character subjects onto all sorts of images that were not character focused; such as scenery shots, item focus shots, transitioning scenes, logos and texts, panning of the environment before a subject walks into the shot, a magical explosion on screen. This incorrect info resulted in generations similar to the ones above, subjects would not appear in the image, or would appear fused into parts of a scenery, special effects shots or be embeded with them, or images with empty settings that look like should've contained someone in it. </p>"},{"location":"pages/udw/udwpipeline/#face-detector-for-fixing-character-tags","title":"Face Detector for fixing character tags","text":"<p>The solution I arrived at was repurposing the anime face detection tool used in the github pipeline, which detects passable images of characters and then creates 1:1 crop stamps of their face for a dataset. Per the github, \"The model detects near-frontal anime faces and predicts 28 landmark points\", meaning that it will use these predefine facial points to detect faces on all images in a directory and give each image a scoring threshold, of which it will give it a passing detection or not. I modified the script to have it make duplicates of all images that pass the face detection threshold into a different folder instead of creating cropped images. These images will be imported after the inital dataset import to my image organizer is completed. I will bulk delete all the subject tags from the initial images and then import the passing copied images with the subject tags still intact and it will update the ones already on file. </p> <p> </p> <p>Images from Hysts' anime face detection tool github</p> <p>My first training after using this method instantly fixed the prompting of subjects. A quick skim of the images that passed the face detection threshold show no signs of false positives, and there is no issue if there are cases of false negatives, as manual review will take care of anything that were missed. Other miscellaneous improves I used for this phase include an autotagger aggregate that will tag based on the average of 3 or more classifiers running concurrently and has the tagging of copyright names removed just so that I reduce false positive tags of characters that don\u2019t belong in the content from being erroneously introduced. </p>"},{"location":"pages/udw/udwpipeline/#dataset-organization-and-preparation","title":"Dataset Organization and Preparation","text":""},{"location":"pages/udw/udwpipeline/#hydrus","title":"Hydrus","text":"<p>While dedicated image organizers for stable diffusion datasets are available, I elected with using an niche desktop application called Hydrus Network. Created in the early 2010s for the purpose of organizing large media collections (of internet memes and other s#!tposts) under a single location with various customizable categories, their features were coincidentally modeled after the format of imageboards such as Danbooru, same as Novel AI. </p> <p>The media is tabulated based on file hash rather than whatever the file is named as. This aspect synergizes not only with how the datasets need to be tagged if following the Booru/NovelAI format, but with how my organization scripts operates on file hash values when sorting unique frames out, as well as how I incorporate face detection copies of images with correct subject tagging.</p> <p></p> <p>Main View of Hydrus. Selected Image and current tags on left, image browser on right.</p> <p>Once my sorted and auto tagged images are completed, I will import the dataset batch into Hydrus and it will associate the tag sidecar txt files generated by the taggers to the images and will automatically populate the datapoints hits. I also include Hydrus specific metadata of the series, episode, and scene for later manual review once the import is finished.</p> <p>Once the first import is complete, I will select the entire batch and edit their tag information to remove all the subject tags (1boy/1girl and other variations of multiple of each for example) in a single click, and then import the copied images from the face detection step. Since the copies have the same hash value, the only thing that will change is that the sidecar txt values will update to include any new values and will not replace any changes I may have already made. Once the subject values have been reintroduced, I will then delete those copies, but will keep the originals of the first import as they will still be required in the future.</p> <p></p> <p>Bottom left previewed image, its current tags above, and tag panel on right with manual corrections ready to apply</p> <p>From here I can proactively check any tags I may have previously had issues with and search for all images by that tag, remove tags if incorrect, maybe even delete images that could be seen as bad data, and overall just skim that the frames I did get were satisfactory. I will repeat this process with all new dataset batches I make until the model is ready to train. From here I can use the Hydrus metadata I included to only select a specific range of data that is ready to go and it will export a new set of copies with sidecar txt files that the trainer will need to associate the tags and images.</p>"},{"location":"pages/udw/udwpipeline/#finetune-training","title":"Finetune Training","text":"<p>Things to note before continuing:</p> <ul> <li> <p>Card used for training with Kohya SD-Script is a water cooled RTX4090 MSI Suprim Liquid X on my personal machine.</p> </li> <li> <p>Training 200k Images at 512x512 resolution at batch size 16 takes 48 hours. </p> </li> <li> <p>Projected Training of 400k images at 768x768 resolution at batch size 1 is 120 hours based on a fellow trainer who trained a similar configuration with a 16GB Google TensorGPU on the cloud, with approximately 168 hours if trained at 1024x1024 over SD1/NAI. </p> </li> <li> <p>From a test I already tried a 768 resolution, batch size at 16 down to 14 caused a CUDA Out of memory error within minutes of training start and then requires a PC restart. Have not tried again with the current pending training data size as I decided to focus on prepping the dataset</p> </li> <li> <p>SDXL training requires even more memory to accurately finetune the model, outside the scope of my hardware.</p> </li> <li> <p>While training the model, I cannot perform any other GPU based tasks on my machine or risk causing an Out of Memory error. It also gets very hot.</p> </li> <li> <p>Due to the above, I focused more on gathering the dataset and do as much quality work as I could before committing to a very lengthy training session.</p> </li> </ul>"},{"location":"pages/udw/udwpipeline/#kohya-sd-scripts","title":"Kohya SD Scripts","text":"<p>Japanese developer Kohya-SS\u2019 SD Script package is an older but consistent training package. While it mostly supports LoRA and other network-based checkpoint trainings, it still supports full finetune training for SD1.x and XL checkpoints. It only needs to be pointed to a training directory, will check the main training folder and the regulation folder if enabled, and will then just follow the training parameters set in the powershell script and output the checkpoint when done. </p> <p></p> <p>SD-Script running the aspect bucket for 768x768 resolutions right before the training step.</p> <p>The default training settings are borrowed from the NAI training settings and includes the aspect ratio bucketing so the resolution sizes are not restricted to 1:1 aspect image, but resolution size will be dictated by hardware used for training so its defaulted to 512x512 but can be adjusted to be higher such as 1024x1024 like SDXL.</p>"},{"location":"pages/udw/udwpipeline/#post-training-review","title":"Post Training Review","text":"<p>After the model is trained, I will run several \u201ctemplates\u201d to test changes between a previous version of the model compared to the current one. I will note if there are any visual improvements, changes in the look of backgrounds, character feature consistencies, detail in non-upscaled generations, and if any errors I caught in the previous cycle were fixed. </p> <p> </p> <p>Same seed, varied results as dataset arrangement is updated through Alpha v7, v8, and v9 respectively.</p> <p>Per the example, you can see that v8's default frame changed quite a bit to portray the subject farther away or smaller. After some tag correction in the the image composition category (Portrait/Face, Upper Body, Cowboy Shot, Full Body, etc) and some new data with preemptive composition tagging, as well as new cropped image with facial close ups, we nudged back close to the v7 perspective with fuller face detail although the subject is not resting the lantern anymore on a foreground surface. It won't always be a similar frame or pose match when try to improve the dataset's quality or concept understandings, but we can correct unexpected behaviors when they surface and do fine tweaking along the way.</p> <p>Along with testing my own seeds and settings, I will also take prompts of other stable diffusion images I see in the wild and run it through this model to test how others prompting styles would translate on UDW's generations. If I finding tags and prompts that don\u2019t work or aren\u2019t producing intended results, I will take note of them to check in Hydrus. </p>"}]}